{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: solid 1px red; margin-bottom: 2% \">\n",
    "\n",
    "## ARCHER2 SCIENTIFIC PYTHON COURSE\n",
    "\n",
    "# Parallel Processing and mpi4py\n",
    "<hr style=\"border: solid 1px red; margin-bottom: -1%; \">\n",
    "\n",
    "## Website:  http://www.archer2.ac.uk \n",
    "\n",
    "## Helpdesk: support@archer2.ac.uk\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"../images/ukri_logo.png\" style=\"float: center\">\n",
    "<br>\n",
    "<img src=\"../images/hpe_logo.png\" style=\"float: center\">\n",
    "<br>\n",
    "<img src=\"../images/epcc_logo.png\" style=\"float: center\">\n",
    "<br>\n",
    "<img src=\"../images/archer2_logo.png\" style=\"float: center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/reusematerial.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## Message Passing with Python (mpi4py)\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "\n",
    "There are a number of message passing interface implementations for python; the most widely used is `mpi4py` (v3.1.3 is the latest).\n",
    "\n",
    "The source is found at\n",
    "https://github.com/mpi4py/mpi4py\n",
    "\n",
    "#### Install\n",
    "\n",
    "mpi4py is already provided on ARCHER2 through the `cray-python` module. If you are undertaking this lesson on ARCHER2 you do not need to install it yourself.\n",
    "\n",
    "`module load cray-python`\n",
    "\n",
    "If you have the Anaconda distribution, it is a relatively straightforward matter to install `mpi4py` and an actual MPI implementation (usually OpenMPI  or MPICH) which is required to do the work underneath. E.g.,\n",
    "\n",
    "```bash\n",
    "local-shell> conda install --channel mpi4py mpich mpi4py\n",
    "```\n",
    "\n",
    "Note for users who already have MPI installed (e.g., for C/Fortran): to prevent name clashes in your `PATH`  it can be useful to use a conda environment to install `mpi4py` and the associated MPI implementation. See\n",
    "http://conda.pydata.org/docs/using/envs.html\n",
    "\n",
    "#### Using MPI in the ipython notebook\n",
    "\n",
    "While it is possible to use `mpi4py` directly via notebook cells, the execution model is not very helpful. If interested, see the `ipyparallel` package. Here, we will use the shell escape to run parallel examples. If you are running on ARCHER2 then you cannot run the mpi4py examples on the login nodes, and will need to use a batch script to submit your jobs to the system.\n",
    "\n",
    "#### Note\n",
    "\n",
    "Python documentation for MPI tends to be (at best) terse. If you are not familiar with the concepts behind MPI, it may be worth trying to familiarise yourself in a different programming context, e.g., C or Fortran, for which materials are generally more detailed.\n",
    "\n",
    "Readers with no prior knowledge of the Message Passing model might like to consult material available at\n",
    "\n",
    "https://www.archer2.ac.uk/training/courses/210317-mpi/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## General\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "\n",
    "The `mpi4py` python interface follows the MPI C++ interface (actually removed from the latest MPI standard). Formally, the MPI standard says nothing about python.\n",
    "\n",
    "Internally, the `mpi4py` _implementation_ uses C.\n",
    "\n",
    "Some appreciation of the object-oriented nature in python is useful.\n",
    "\n",
    "The C++ names can be - annoyingly - slightly different from the corresponding C/Fortran bindings.\n",
    "They are often reversed, e.g.,\n",
    "\n",
    "```\n",
    "MPI_Buffer_attach() becomes MPI.Attach_buffer()\n",
    "```\n",
    "\n",
    "The table on C++ bindings in the MPI standard (annex) can provide a useful quick reference.\n",
    "\n",
    "ipython `help(mpi4py.MPI)` is rather long, so it can be helpful to narrow the search by e.g., `help(mpi4py.MPI.Intracomm)`. However this only works if you know what you are looking for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## MPI Environment\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "\n",
    "Python code using message passing should import the `mpi4py` python module, typically,\n",
    "\n",
    "```py\n",
    "from mpi4py import MPI\n",
    "```\n",
    "\n",
    "The `import mpi4py` statement is responsible for initialising MPI (if not already initialised)\n",
    "so there is no analogue of calls to `MPI_Init()` and `MPI_Finalize()` in typical python code.\n",
    "\n",
    "### `MPI.COMM_WORLD`\n",
    "\n",
    "All communicators are python objects, and the pre-defined communicator is\n",
    "`COMM_WORLD` accessed via the `MPI` module, e.g.,\n",
    "\n",
    "```py\n",
    "rank = MPI.COMM_WORLD.rank    # May also see comm.Get_rank()\n",
    "size = MPI.COMM_WORLD.size    # May also see comm.Get_size()\n",
    " ```\n",
    "\n",
    "\n",
    "\n",
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load example1.py\n",
    "\"\"\"Importing and using MPI.COMM_WORLD\"\"\"\n",
    "\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main(comm):\n",
    "    \"\"\"Report rank and size of this communicator\"\"\"\n",
    "    rank = comm.rank\n",
    "    size = comm.size\n",
    "    sys.stdout.write(\"Hello from rank {:2d} of {:2d}\\n\".format(rank, size))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Execute in MPI.COMM_WORLD\"\"\"\n",
    "    main(MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While one may load the examples for inspection (as above),\n",
    "# cells themselves execute in serial.\n",
    "\n",
    "# Use the shell escape to run the script via srun\n",
    "!srun -n 8 python example1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## Point-to-Point Messages\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "\n",
    "The interface makes significant use of optional arguments:\n",
    "\n",
    "```py\n",
    "Comm.Send(self, buf, int dest, int tag=0)\n",
    "Comm.Recv(self, buf, int source=ANY_SOURCE, int tag=ANY_TAG,\n",
    "          Status status=None) \n",
    "```\n",
    "\n",
    "Note **upper case** in `Send()` and `Recv()`: these are for \"buffer-like\" data.\n",
    "\n",
    "There are also distinct `send()` and `recv()` methods; see below.\n",
    "\n",
    "There are no count or data type arguments associated with the message buffer `buf` (at first sight).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Point-to-point messages using `numpy.ndarray` buffers\n",
    "\n",
    "A numerical application might typically wish to communicate contiguous arrays of data:\n",
    "\n",
    "\n",
    "```py\n",
    "import numpy\n",
    "sz = 1000\n",
    "buf = numpy.zeros(sz, type = numpy.double)\n",
    "\n",
    "if rank == 0:\n",
    "    MPI.COMM_WORLD.Send([buf, sz, MPI.DOUBLE], 1, tag = 99)\n",
    "elif rank == 1:\n",
    "    MPI.COMM_WORLD.Recv([buf, sz, MPI.DOUBLE], source = 0, \\\n",
    "                        tag = 99)\n",
    "```\n",
    "\n",
    "Here the count and data type are explicitly specified as part of a list and can be omitted.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Example: Blocking Send/Recv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load example3.py\n",
    "\"\"\"A simple blocking Send/Recv pair\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main(comm, sz):\n",
    "    \"\"\"Send a message between ranks 0 and 1\"\"\"\n",
    "    rank = comm.Get_rank()\n",
    "\n",
    "    if rank == 0:\n",
    "        msg = numpy.ones(sz, numpy.double)\n",
    "        comm.Ssend([msg, sz, MPI.DOUBLE], dest = 1, tag = 999)\n",
    "    elif rank == 1:\n",
    "        msg = numpy.zeros(sz, numpy.double)\n",
    "        comm.Recv([msg, sz, MPI.DOUBLE], source = 0, tag = 999)\n",
    "        sys.stdout.write(\"Rank 1 received {}\".format(msg))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sz = 4\n",
    "    main(MPI.COMM_WORLD, sz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank [0] received messages from ranks 1 and 1\n",
      "Rank [1] received messages from ranks 0 and 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsh\\OneDrive - University of Edinburgh\\ARCHER2\\RSECon22\\PythonHPC\\notebooks\\mpi4py\\example3.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  smsg = numpy.array([comm.rank], numpy.int)\n",
      "C:\\Users\\dsh\\OneDrive - University of Edinburgh\\ARCHER2\\RSECon22\\PythonHPC\\notebooks\\mpi4py\\example3.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  smsg = numpy.array([comm.rank], numpy.int)\n",
      "C:\\Users\\dsh\\OneDrive - University of Edinburgh\\ARCHER2\\RSECon22\\PythonHPC\\notebooks\\mpi4py\\example3.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  rmsg = numpy.zeros(2, numpy.int)\n",
      "C:\\Users\\dsh\\OneDrive - University of Edinburgh\\ARCHER2\\RSECon22\\PythonHPC\\notebooks\\mpi4py\\example3.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  rmsg = numpy.zeros(2, numpy.int)\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 2 python ./example3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## Non-Blocking Point-to-Point Messages\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "\n",
    "Non-blocking (\"immediate\") methods are of the form:\n",
    "\n",
    "```py\n",
    "Comm.Isend(self, buf, int dest, int tag=0)\n",
    "Comm.Irecv(self, buf, int source=ANY_SOURCE, int tag=ANY_TAG)\n",
    "```\n",
    "\n",
    "Both these return objects of the `Request` class. \n",
    "\n",
    "Request objects are handled either by instance methods, e.g.,\n",
    "```py\n",
    "request.Wait(self, Status status=None)\n",
    "```\n",
    "\n",
    "or class methods, e.g.,\n",
    "```py\n",
    "Request.Waitall(type cls, requests, statuses=None)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "### Example: Non-blocking communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load example4.py\n",
    "\"\"\"An Example of non-blocking Isend/Irecv pairs\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main(comm):\n",
    "    \"\"\"Exchange messages with 'ajoining' ranks\"\"\"\n",
    "    p1 = comm.rank + 1\n",
    "    m1 = comm.rank - 1\n",
    "    if p1 >= comm.size: p1 = 0\n",
    "    if m1 < 0: m1 = comm.size - 1\n",
    "\n",
    "    smsg = numpy.array([comm.rank], numpy.int)\n",
    "    rmsg = numpy.zeros(2, numpy.int)\n",
    "\n",
    "    reqs1 = comm.Issend(smsg, p1)\n",
    "    reqs2 = comm.Issend(smsg, m1)\n",
    "    reqr1 = comm.Irecv(rmsg[0:], source = p1)\n",
    "    reqr2 = comm.Irecv(rmsg[1:], source = m1)\n",
    "\n",
    "    reqr1.Wait(); reqr2.Wait()\n",
    "    sys.stdout.write(\"Rank {} received messages from ranks {} and {}\\n\"\\\n",
    "                         .format(smsg, rmsg[1], rmsg[0]))\n",
    "\n",
    "    MPI.Request.Waitall([reqs1, reqs2])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main(MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! srun -n 4 python ./example4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## Python objects as messages\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "\n",
    "As python objects can be serialised, message passing is available:\n",
    "\n",
    "```py\n",
    "Comm.send(self, obj, int dest, int tag=0)\n",
    "Comm.recv(self, buf=None, int source=ANY_SOURCE, int tag=ANY_TAG,\n",
    "          Status status=None)  \n",
    "```\n",
    "\n",
    "Note **lower case** `send()` and `recv()`.\n",
    "\n",
    "Serialisation and deserialisation carry an overhead in memory and time; complex/large objects may be slow.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Example: sending a list\n",
    "\n",
    "Note the incoming message is received as the return value\n",
    "\n",
    "```py\n",
    "msg = comm.recv(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load example5.py\n",
    "\"\"\"Message passing a python object\"\"\"\n",
    "\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main(comm):\n",
    "    \"\"\"Send a list from rank 0 to rank 1\"\"\"\n",
    "\n",
    "    if comm.rank == 0:\n",
    "        msg = [\"Any\", \"old\", \"thing\", comm.rank, {\"size\" : comm.size}]\n",
    "        comm.send(msg, dest = 1, tag = 999)\n",
    "    elif comm.rank == 1:\n",
    "        msg = comm.recv(source = 0, tag = 999)\n",
    "        sys.stdout.write(\"Rank 1 received {}\\n\".format(msg))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main(MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! srun -n 2 python ./example5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## Collective Communication\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "\n",
    "Collective operations are implemented as methods on `Comm`, e.g.,\n",
    "\n",
    "```py\n",
    "Comm.Bcast(self, buf, int root=0)\n",
    "```\n",
    "\n",
    "Use `numpy.ndarray` as the actual data, or other contiguous buffer.\n",
    "\n",
    "Again, note **upper case** `Bcast()`. Distinct `bcast()` etc are intended for python objects.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Reductions\n",
    "\n",
    "Reductions involve the `Op` class, e.g.,\n",
    "\n",
    "```py\n",
    "Comm.Reduce(self, sendbuf, recvbuf, Op op=SUM, int root=0)\n",
    "```\n",
    "\n",
    "Operations include `MIN`, `MAX`, `SUM` and so on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main(comm):\n",
    "    \"\"\"The sum of a ranks is...\"\"\"\n",
    "\n",
    "    total_sum = numpy.array(0, 'i')\n",
    "    rank = numpy.array(comm.rank, 'i')\n",
    "    comm.Reduce([rank, MPI.INTEGER], [total_sum, MPI.INTEGER], op=MPI.SUM, root=0)\n",
    "\n",
    "    if comm.rank == 0:\n",
    "        sys.stdout.write(\"Sum of ranks: {}\\n\".format(total_sum))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main(MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reductions with python objects\n",
    "\n",
    "\n",
    "The equivalent method for python objects is:\n",
    "\n",
    "```py\n",
    "Comm.reduce(self, sendobj, op=SUM, int root=0)\n",
    "```\n",
    "\n",
    "Built-in `Op` operations rely on the relevant special method [e.g., `__add__()`] being implemented for `sendobj`.\n",
    "\n",
    "You can create your own `Op` functions.\n",
    "\n",
    "Standard python types should behave \"as expected\".\n",
    "\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load example6.py\n",
    "\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main(comm):\n",
    "    \"\"\"The sum of a list is...\"\"\"\n",
    "\n",
    "    mylist = []\n",
    "    mylist = comm.Reduce([comm.rank], op=MPI.SUM, root=0)\n",
    "\n",
    "    if comm.rank == 0:\n",
    "        sys.stdout.write(\"List of ranks: {}\\n\".format(mylist))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main(MPI.COMM_WORLD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! srun -n 4 python ./example6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## Communicators\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "A communicator provides the context within which communication will occur.\n",
    "\n",
    "\n",
    "The class heirarchy for communicator objects is\n",
    "\n",
    "```\n",
    "Comm\n",
    "    Intracomm\n",
    "    Intercomm\n",
    "        Topocomm\n",
    "            Cartcomm\n",
    "            Distgraphcomm\n",
    "            Graphcomm\n",
    "```\n",
    "\n",
    "Many methods are implemented on `Comm` and inherited by subclasses, e.g.,\n",
    "\n",
    "```\n",
    "    comm.Barrier()\n",
    "```\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Example\n",
    "\n",
    "Creating a Cartesian communicator `Cartcomm` object from an existing `Intracomm` object uses:\n",
    "```\n",
    "Intracomm.Create_cart(self, dims, periods=None, reorder=False)\n",
    "```\n",
    "\n",
    "and will return a new Cartesian communicator.\n",
    "\n",
    "Methods for this object include `Get_cart_rank()`, `Get_coords()` and so on.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load example2.py\n",
    "\"\"\"Creating a Cartesian Communicator\"\"\"\n",
    "\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main(parent):\n",
    "    \"\"\"Create a 2-d Cartesian communicator from parent communicator\"\"\"\n",
    "\n",
    "    dims = MPI.Compute_dims(parent.size, 2)\n",
    "\n",
    "    comm = parent.Create_cart(dims, periods = [True, False])\n",
    "    rank = comm.Get_rank()\n",
    "    coords = comm.Get_coords(rank)\n",
    "    upx = comm.Shift(0, 1)\n",
    "    upy = comm.Shift(1, 1)\n",
    "\n",
    "    out = \"Rank{:2d} coords{:2d} {:2d} upx(src,dst) {} upy(src,dst) {}\\n\"\\\n",
    "                         .format(rank, coords[0], coords[1], upx, upy)\n",
    "    sys.stdout.write(out)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Execute in MPI.COMM_WORLD\"\"\"\n",
    "    main(MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srun -n 4 python ./example2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## Other features\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "\n",
    "Standard MPI functionality is available (`mpi4py` version 2.0):\n",
    "\n",
    "* `Comm.Abort()`, `MPI.Wtime()`, ...\n",
    "\n",
    "\n",
    "* User-defined types (methods implemented in the `Datatype` class)\n",
    "\n",
    "\n",
    "* Single-sided communication (see the `Win` class)\n",
    "\n",
    "\n",
    "* MPI-IO (see the `File` class)\n",
    "\n",
    "\n",
    "* a few functions are not implemented, e.g., `AlltoAllw()`\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Some other considerations\n",
    "\n",
    "* Large numbers of MPI tasks can mean that the action of using `import` to load libraries across all these tasks can take a long time and slow down the starting of the parallel program. \n",
    "\n",
    "\n",
    "* Can start in python and call another language\n",
    "    - can typically pass a communicator and other relevant data\n",
    "    - convenient access to C `MPI_Comm` handles requires `mpi4py` 2.0.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "help(MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## ARCHER2 Exercise: Compare `Send()` and `send()`\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Logging in\n",
    "\n",
    "With your guest account:\n",
    "```bash\n",
    "local-shell> ssh -X username@login.archer2.ac.uk\n",
    "```\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### The `Send()` example `exercise1.py`\n",
    "\n",
    "This python script measures the time taken to pass a message back and forth between two MPI tasks. If we know the amount of data transferred, and the time taken, we can work out the bandwidth in bytes/second. In the\n",
    "limit of zero message size, this time is the latency - the time taken to transfer a message at all.\n",
    "\n",
    "Have a look at the script `exercise1.py` and check exactly what the `Send()` - actually `Ssend()` - and `Recv()` are doing.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Running the Send() version\n",
    "\n",
    "Compute jobs on ARCHER2 must be submitted to the queue system. A submission script is provided. This should look like:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash --login\n",
    "\n",
    "#SBATCH --job-name mpi4py\n",
    "#SBATCH --nodes=1              # Request one node\n",
    "#SBATCH --tasks-per-node=2     # Request two tasks per node\n",
    "#SBATCH --cpus-per-task=1      # Request one cpu per task\n",
    "#SBATCH --time=00:20:00\n",
    "\n",
    "#SBATCH --account=ta054\n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --qos=short\n",
    "\n",
    "module load cray-python\n",
    "\n",
    "# Run with two MPI tasks \n",
    "srun python exercise1.py\n",
    "```\n",
    "\n",
    "Note that two relevant modules are loaded to provide access to python on the back end. For further information see\n",
    "\n",
    "https://docs.archer2.ac.uk/user-guide/python/\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Submitting the script\n",
    "\n",
    "The `bash` script (not the python script) is submitted to the queue system using\n",
    "```bash\n",
    "guest123@login001:> sbatch <script-name>\n",
    "```\n",
    "\n",
    "Run the example script and check the output; this should report the time taken to exchange messages of a given size bettwen two MPI tasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Change the placement of the MPI tasks (optional)\n",
    "\n",
    "Try altering the submission script so that the two MPI tasks each run on a different node. The altered script should include the changes:\n",
    "```bash\n",
    "#SBATCH --nodes=2              # Request two nodes\n",
    "#SBATCH --tasks-per-node=1     # Request one task per node\n",
    "...\n",
    "srun python ./exercise1.py\n",
    "```\n",
    "\n",
    "Can you see if there is any difference in the reported results?\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Rewrite the python example so that is uses `send()` and `recv()` rather than `Send()` and `Recv()`.\n",
    "\n",
    "Instead of using a `numpy.ndarray` object for the message data, try using a standard python list containing the required number of data items.\n",
    "\n",
    "What influence does this have on the measured bandwidth?\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr class=\"top\">\n",
    "\n",
    "## Summary\n",
    "\n",
    "<hr class=\"bot\">\n",
    "<br>\n",
    "\n",
    "* `mpi4py` can be used to write parallel python programs\n",
    "\n",
    "\n",
    "* Can be used to interact with other programs using suitable communicator\n",
    "  \n",
    "<br>\n",
    "\n",
    "* Useful links\n",
    "\n",
    "  * https://mpi4py.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br>\n",
    "<hr class=\"top\">\n",
    "<hr class=\"bot\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
